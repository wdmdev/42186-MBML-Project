{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoDelta, AutoNormal\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO, config_enumerate\n",
    "from pyro.contrib.tracking.extended_kalman_filter import EKFState\n",
    "from pyro.contrib.tracking.distributions import EKFDistribution\n",
    "from pyro.contrib.tracking.dynamic_models import NcvContinuous\n",
    "from pyro.contrib.tracking.measurements import PositionMeasurement\n",
    "import pyro.poutine as poutine\n",
    "from pyro.contrib.forecast import ForecastingModel, Forecaster, eval_crps\n",
    "from pyro.infer.reparam import LocScaleReparam, AutoReparam\n",
    "from pyro.ops.stats import quantile\n",
    "\n",
    "from main import get_energy_data, get_weather_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pyro.set_rng_seed(20200928)\n",
    "torch.manual_seed(20200101)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfW.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = None # number of time steps to include, if None all data is used\n",
    "test_frac = 0.1 # fraction of data to use for testing\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(\"preprocessed_data/df.csv\")#.iloc[:1000]\n",
    "df[\"time_str\"] = [d.split(\"+\")[0] for d in df[\"time_str\"]]\n",
    "df[\"time_str\"] = pd.to_datetime(df[\"time_str\"], infer_datetime_format=True)#'%Y-%m-%d %H:%M:%S.%f') # 2015-01-01 10:00:00+00:00\n",
    "df = df.groupby(\"time_str\").mean()\n",
    "df = df.reset_index().set_index('time_str')\n",
    "df.index.name = None\n",
    "df = df.asfreq(\"D\") # aggregate data, can be commented out to use all data\n",
    "dfW = get_weather_data(df)\n",
    "dfE = get_energy_data(df)\n",
    "\n",
    "\n",
    "# start, train end, test end\n",
    "n = len(df) if T is None else T\n",
    "T0 = 0\n",
    "T1 = n - int(test_frac*n) # 10 % test data\n",
    "T2 = n\n",
    "print(\"Number of observations:\", n)\n",
    "print(\"Number of training observations:\", T1 - T0)\n",
    "print(\"Number of test observations:\", T2 - T1)\n",
    "\n",
    "# convert to torch tensors\n",
    "# X_W = torch.from_numpy(dfW.values).float()\n",
    "# X_E = torch.from_numpy(dfE.values).float()\n",
    "# obs = torch.from_numpy(df[\"price actual\"].values).float()\n",
    "\n",
    "# standardize data\n",
    "df_standardized = dfE.merge(dfW, left_index=True, right_index=True)\n",
    "df_standardized[\"price_ actual\"] = df[\"price actual\"]\n",
    "scaler = StandardScaler().fit(df_standardized[:T1])\n",
    "df_standardized = scaler.transform(df_standardized)\n",
    "df_standardized = pd.DataFrame(df_standardized, columns=list(dfW.columns) + list(dfE.columns) + [\"price_actual\"], index = df.index)\n",
    "# print(df_standardized.columns)\n",
    "\n",
    "# create tensors\n",
    "X_W = torch.from_numpy(df_standardized[dfW.columns].values).float()\n",
    "X_E = torch.from_numpy(df_standardized[dfE.columns].values).float()\n",
    "obs = torch.from_numpy(df_standardized[\"price_actual\"].values).float()\n",
    "\n",
    "# translate to pyro forecasting language\n",
    "y = obs.unsqueeze(-1) if T is None else obs[-T:].unsqueeze(-1)\n",
    "covariates = torch.hstack((X_E, X_W)) if T is None else torch.hstack((X_E, X_W))[-T:]\n",
    "# covariates = X_E if T is None else X_E[-T:]\n",
    "\n",
    "# define constants\n",
    "n, p = covariates.shape\n",
    "dates = df.index[-n:] # used for plotting\n",
    "\n",
    "\n",
    "\n",
    "# covariates[:T1].mean(axis = 0), covariates.std(axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR_DLM(ForecastingModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tau = 1\n",
    "        self.M = 4\n",
    "        # self.h_dim = X_E.shape[1]\n",
    "        self.h_dim = 20\n",
    "        self.gamma = 1\n",
    "        self.lambda_f = 1\n",
    "\n",
    "    def model(self, zero_data, covariates):\n",
    "        x_E = covariates[:, :X_E.shape[1]]\n",
    "        x_W = covariates[:, -X_W.shape[1]:]\n",
    "        # covariates = x_E\n",
    "\n",
    "        data_dim = zero_data.size(-1)\n",
    "        feature_dim = covariates.size(-1)\n",
    "\n",
    "        theta_f = pyro.sample(\"theta_f\", dist.Normal(0, self.lambda_f).expand((feature_dim, self.h_dim)).to_event(2))\n",
    "        theta_g = pyro.sample(\"theta_g\", dist.Normal(0, 1).expand(self.h_dim).to_event(1))\n",
    "\n",
    "        with self.time_plate:\n",
    "            \n",
    "            h_mean = (theta_f*covariates.unsqueeze(-1)).sum(-2, keepdim=True).squeeze(-2)\n",
    "            with poutine.reparam(config={\"h_param\": AutoReparam()}):\n",
    "            # print(covariates.unsqueeze(-1).shape, theta_f.shape)\n",
    "            \n",
    "            # print(\"h_mean: \", h_mean.shape, \"R: \",R.shape)\n",
    "                h = pyro.sample(\"h\", dist.Normal(h_mean, R).to_event(1))\n",
    "        pyro.deterministic(\"h_data\", h)\n",
    "\n",
    "        # prediction \n",
    "        # print(\"theta_g: \", theta_g.shape, \"h: \", h.shape)\n",
    "        prediction = (theta_g*h).sum(-1, keepdim=True)\n",
    "        assert prediction.shape[-2:] == zero_data.shape\n",
    "        # record in model_trace\n",
    "        pyro.deterministic(\"prediction\", prediction)\n",
    "\n",
    "        sigma = pyro.sample(\"sigma\", dist.HalfCauchy(self.tau).expand([1]).to_event(1))\n",
    "        noise_dist = dist.Normal(0, sigma)\n",
    "\n",
    "        self.predict(noise_dist, prediction)\n",
    "\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.clear_param_store()\n",
    "model = AR_DLM()\n",
    "guide = AutoNormal(poutine.block(model, hide=[f\"z_{i}\" for i in range(T1+1)]))\n",
    "# optim = Adam({ 'lr': 1e-3 })\n",
    "forecaster = Forecaster(\n",
    "    model,\n",
    "    y[:T1],\n",
    "    covariates[:T1],\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.05,\n",
    "    num_steps=1000,\n",
    "    guide = guide,\n",
    "    # optim = optim,\n",
    ")\n",
    "\n",
    "with poutine.trace() as tr:\n",
    "    forecaster(y[:T1], covariates, num_samples=100)\n",
    "\n",
    "# extract the values from the recorded trace\n",
    "posterior_samples = {\n",
    "    name: site[\"value\"]\n",
    "    for name, site in tr.trace.nodes.items()\n",
    "    if site[\"type\"] == \"sample\"\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(dates, y, label=\"observed\", alpha= .4)\n",
    "pred_p10, pred_p50, pred_p90 = quantile(posterior_samples['prediction'], (0.1, 0.5, 0.9)).squeeze(-1)\n",
    "ax.plot(dates[:T1], pred_p50[:T1], label=\"train prediction\", alpha = .4)\n",
    "ax.plot(dates[T1:], pred_p50[T1:], label=\"test prediction\", alpha = .4)\n",
    "ax.legend()\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation = 45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "resid = obs - posterior_samples['prediction'].squeeze()\n",
    "print(\"Mean price: \", obs.mean())\n",
    "print(\"Mean abs residual: \", resid.abs().mean())\n",
    "print(\"Mean abs train residual: \", resid[:, :T1].abs().mean())\n",
    "print(\"Mean abs test residual: \", resid[:, T1:].abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.clear_param_store()\n",
    "model = VanillaDLM()\n",
    "forecaster = Forecaster(\n",
    "    model,\n",
    "    y[:T1],\n",
    "    covariates[:T1],\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.05,\n",
    "    num_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)\n",
    "# record all latent variables in a trace\n",
    "with poutine.trace() as tr:\n",
    "    forecaster(y[:T1], covariates, num_samples=100)\n",
    "\n",
    "# extract the values from the recorded trace\n",
    "posterior_samples = {\n",
    "    name: site[\"value\"]\n",
    "    for name, site in tr.trace.nodes.items()\n",
    "    if site[\"type\"] == \"sample\"\n",
    "}\n",
    "\n",
    "# posterior_samples[\"prediction\"].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract percentiles of the posterior samples\n",
    "pred_p10, pred_p50, pred_p90 = quantile(posterior_samples['prediction'], (0.1, 0.5, 0.9)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(dates, y, label=\"observed\", alpha= .4)\n",
    "ax.plot(dates[:T1], pred_p50[:T1], label=\"train prediction\", alpha = .4)\n",
    "ax.plot(dates[T1:], pred_p50[T1:], label=\"test prediction\", alpha = .4)\n",
    "ax.legend()\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation = 45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = obs - posterior_samples['prediction'].squeeze()\n",
    "\n",
    "resid_p10, resid_p50, resid_p90 = quantile(resid, (0.1, 0.5, 0.9)).squeeze(-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# ax.plot(dates, y, label=\"observed\", alpha= .4)\n",
    "ax.plot(dates[:T1], resid_p50[:T1], label=\"train prediction\", alpha = .4)\n",
    "ax.plot(dates[T1:], resid_p50[T1:], label=\"test prediction\", alpha = .4)\n",
    "# ax.fill_between(dates, resid_p10, resid_p90, color=\"red\", alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title(\"Residuals, mean abs residual: {:.2f}\".format(resid.abs().mean()))\n",
    "\n",
    "print(\"Mean price: \", obs.mean())\n",
    "print(\"Mean abs residual: \", resid.abs().mean())\n",
    "print(\"Mean abs train residual: \", resid[:, :T1].abs().mean())\n",
    "print(\"Mean abs residual: \", resid[:, T1:].abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resid[T1:]\n",
    "resid.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dce29682316a73f3dbf4a78d3a346bd34e7f1893b4db7c16187fa8dc60916ec6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('MBML-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
